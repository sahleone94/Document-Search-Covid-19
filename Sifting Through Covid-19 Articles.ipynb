{"cells":[{"metadata":{},"cell_type":"markdown","source":"This notebook takes a a quick look at Covid-19 article dataset on Kaggle https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge  \nIt focuses on different ways of sifting through the articles.  \nThe noteboook is separated as such \n\n1. Loading and Cleaning\n2. Keyword Search\n3. Topic Modeling\n4. Cosine Similarity"},{"metadata":{},"cell_type":"markdown","source":"# Import Packages"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install --upgrade langdetect -q # install language detection\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport json # read json files\nimport glob #find pathnames\nfrom random import sample # for sampling\n\nimport nltk #natural language processing\nfrom nltk.corpus import stopwords\n\nnltk.download('punkt')\nnltk.download('stopwords')\nnltk.download('wordnet')\n\n                        \nfrom gensim import corpora #Create Corprea\nfrom gensim.models.ldamodel import LdaModel #LDA model\n\nimport pyLDAvis.gensim #Display Topics\n\n        \nstop_words =  set(stopwords.words('english'))\nstop_words.update(['et', 'al',\"addition\", \"respectively\", \"found\", \"although\",'present',\n                  'identified','Thu','Finally','either','suggesting','include',\"well\", \n                   \"associated\", \"method\", \"result\",'used','doi','display',\n                  'https','copyright', 'holder','org','author','available','made','peer',\n                  'reviewed','without','permission','license','rights','reserverd','Furthermore'\n                  'using','preprint','allowed','following','may','thus','funder','International',\n                 'granted','compared','will','one','two','use','different','likely','Discussion',\n                 'medRexiv','Introduction','Moreover','known','funder','6','7','8','paywall' ,\n                       'downarrow','textstyle','1','2','3','4','5','9','10','0','include','number','work','begin','fig','show'])\n        \n        \n\n\n","execution_count":15,"outputs":[{"output_type":"stream","text":"^C\n\u001b[31mERROR: Operation cancelled by user\u001b[0m\n[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# Loading Files"},{"metadata":{"trusted":true},"cell_type":"code","source":"#The files while they have some structure are not completely uniform so pandas.read_json() will not work in this case. For example one article could have one author and another five\n#the Schema\n\n#with open(\"/kaggle/input/CORD-19-research-challenge/json_schema.txt\",'r') as f:\n#    file = f.read()\n#    print(file)\n","execution_count":16,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The following class Kaggle_Covid_19 contains the code for ETL of the data. there is also a method for the keyword search. (A class was creater beacuse the code was getting to repetative)"},{"metadata":{"trusted":true},"cell_type":"code","source":"class Kaggle_Covid_19:\n    '''Created to easy the ETL/EDA Kaggle Covid_19 dataset from:\n    https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge'''\n    \n    \n    def __init__(self, filepath,num_of_articles=1000):\n        self.filepath = filepath\n        self.num_of_articles = num_of_articles\n        \n        \n        \n        \n    def _article_paper_id_title(self,article):\n        '''Create a list of a paper's ID and title''' \n\n        metadata = [article[\"paper_id\"],article[\"metadata\"][\"title\"]]\n        return  metadata\n\n    \n    def _article_authors(self,article):\n        '''Create a list of a paper's ID, authors and source'''\n        authors = []\n\n        for idx in range(len(article[\"metadata\"][\"authors\"])):\n            author = [article[\"paper_id\"],article[\"metadata\"][\"authors\"][idx][\"first\"], article[\"metadata\"][\"authors\"][idx][\"last\"]]\n            authors.append(author)\n    \n        return authors\n\n    \n    def _article_text(self,article):\n        '''Create a list of a paper's ID, and abstracts and body text'''\n        import numpy as np\n        text = article[\"metadata\"][\"title\"]\n        abstract = ''\n    \n        try:\n            for idx in range(len(article[\"abstract\"])):\n                abstract = abstract + '\\n\\n' + article[\"abstract\"][idx][\"text\"]\n            abstract = abstract.strip()\n        except: abstract = ''\n        \n        for idx in range(len(article[\"body_text\"])):\n            text = text + '\\n\\n' + article[\"body_text\"][idx][\"section\"] + '\\n\\n' + article[\"body_text\"][idx][\"text\"]\n        text = text.strip()\n    \n        article_text = [article[\"paper_id\"],text,abstract]\n    \n        return article_text\n\n    \n    def load_files(self):\n    \n        '''(Kaggle) Takes in filepath and returns three dataframe with:\n        1.[\"Paper_Id\",'Text','Abstract']\n        2.[\"Paper_Id\",\"Title\"]\n        3.[\"Paper_Id\",\"First_Name\",\"Last_Name\"]'''\n    \n\n    \n        filepaths = glob.glob(self.filepath,recursive = True) #getting file paths\n    \n        \n        articles = sample(filepaths,self.num_of_articles) #taking a sample as the corpus is to large (for Kaggle)\n    \n    \n        #initialize lists\n        titles_list = []\n        authors_list = []\n        text = []\n    \n        #Filling lists\n        for article in articles:\n            article = json.load(open(article, 'rb'))\n            titles_list.append(self._article_paper_id_title(article))\n            authors_list.extend([*self._article_authors(article)])              \n            text.append(self._article_text(article)) \n    \n        #Transform lists into DataFrames\n            \n        self.authors = pd.DataFrame(authors_list,columns = [\"Paper_Id\",\"First_Name\",\"Last_Name\"])\n        self.titles  = pd.DataFrame(titles_list,columns = [\"Paper_Id\",\"Title\"])\n        self.texts   = pd.DataFrame(text,columns = [\"Paper_Id\",'Text','Abstract'])\n        \n        self._to_clean = pd.concat([self.texts[\"Paper_Id\"],self.texts['Text'] + self.texts['Abstract']],axis = 1 )\n        self._to_clean.columns =[\"Paper_Id\",'Full_Text']\n\n\n\n    def _check_lang(self,text,seed=0):\n        '''Check the language of text'''\n        \n        from langdetect import detect, DetectorFactory\n        DetectorFactory.seed = seed\n        \n        try:\n            return detect(text)\n    \n        except: return None\n    \n    \n    def _tokenize(self,text):\n    \n        '''Cleans Text Data by:\n        1.Remove stopwords\n        2.Remove punctuation\n        3.Normalize text\n        4.Lemmatize text'''\n    \n        from nltk.stem.wordnet import WordNetLemmatizer\n        import re\n        from nltk.tokenize import word_tokenize\n    \n        lemmatizer = WordNetLemmatizer()\n        \n        # normalize case and remove punctuation\n        text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text.lower())\n    \n        # tokenize text\n        tokens = word_tokenize(text)\n    \n        # lemmatize(noun) and remove stop words\n        lem = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n        \n        # lemmatize(verb) and remove stop words\n        lem = [lemmatizer.lemmatize(word,pos = 'v') for word in lem ]\n        \n        # lemmatize(adjective) and remove stop words\n        tokens  = [lemmatizer.lemmatize(word,pos = 'a') for word in lem ]\n\n        return tokens\n\n\n    def clean_text(self):\n        \n        '''Produces a list of cleaned text data (load_files should be run first)'''\n        import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n        #Combine all text data into one column\n  \n        lang = []\n        \n        for text in self._to_clean['Full_Text']:\n            if self._check_lang(text) != None:\n                lang.append(self._check_lang(text))\n            else:lang.append(None)\n    \n        self._to_clean['Language'] = pd.Series(lang)\n        \n        #Remove articles that are not in english\n        self._to_clean = self._to_clean[self._to_clean['Language'] == 'en']\n\n        #Remove the Language column \n        self._to_clean = self._to_clean.drop('Language',axis = 1)\n        #clean text data\n        _clean_text = self._to_clean['Full_Text'].apply(self._tokenize) \n        self.clean_text = pd.concat([self._to_clean['Paper_Id'],_clean_text],axis = 1)\n        del self._to_clean\n        \n        return self.clean_text\n\n    \n    def keyword_Search(self,keywords):\n        '''Searches data set for articles containing keywords.\n    \n        articles: Article dataframe from load_files function\n           \n        clean_text : The output of the clean_data fuction.\n        \n        keywords: A list of words to search for in the articles'''\n    \n   \n        keywords = self._tokenize(' '.join(keywords))\n    \n        papers = []\n\n        for word in keywords:\n            for idx,row in self.clean_text.iterrows():\n                try:\n                    if word in row['Full_Text']:\n                         papers.append(row['Paper_Id'])\n                except: TypeError\n        \n        return papers\n    ","execution_count":17,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"file_path = \"/kaggle/input/\" + \"**/*.json\" \n\ncovid = Kaggle_Covid_19(file_path,num_of_articles=1000)\ncovid.load_files() \narticles = covid.clean_text() # returns a df with the Paper id's and the tokenized cleaned text\n","execution_count":18,"outputs":[{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-18-e918bf4da25a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcovid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKaggle_Covid_19\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_of_articles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m25000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mcovid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0marticles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcovid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclean_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# returns a df with the Paper id's and the tokenized cleaned text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-17-89122111dffb>\u001b[0m in \u001b[0;36mload_files\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;31m#Filling lists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0marticle\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marticles\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0marticle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marticle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m             \u001b[0mtitles_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_article_paper_id_title\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marticle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mauthors_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_article_authors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marticle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/json/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    291\u001b[0m     \u001b[0mkwarg\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0motherwise\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mJSONDecoder\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mused\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m     \"\"\"\n\u001b[0;32m--> 293\u001b[0;31m     return loads(fp.read(),\n\u001b[0m\u001b[1;32m    294\u001b[0m         \u001b[0mcls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobject_hook\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobject_hook\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m         \u001b[0mparse_float\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparse_float\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparse_int\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparse_int\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"metadata":{},"cell_type":"markdown","source":"The number of topics chosen for the LDA is usually done through trail and error. I prefer to use KMeans as a starting point "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n\n# initialize tf-idf vectorizer \nvectorizer = TfidfVectorizer()\n# compute tf-idf values\ntfidf_matrix = vectorizer.fit_transform([' '.join(doc) for doc in articles['Full_Text']])\n\n# Import k-means to perform clusters analysis\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt  # data visualization\nfrom sklearn.metrics import silhouette_score\n\n#init number of clusters\nclus_num = 10\nsil_score = []\n\nSum_of_squared_distances = []\n \nfor k in range(2,clus_num):\n    kmeans = KMeans(n_clusters = k).fit(tfidf_matrix)\n    labels = kmeans.labels_\n    sil_score.append(silhouette_score(tfidf_matrix, labels, metric = 'euclidean'))\n    \nplt.plot(range(2,clus_num), sil_score, 'bx-')\nplt.xlabel('clusters')\nplt.ylabel('silhouette score')\nplt.title('silhouette Method For Optimal k')\nplt.show()\n\n#the highest point denotes the number of clusters","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Topic modeling"},{"metadata":{},"cell_type":"markdown","source":"Topic Modeling is an unsupervised natural language processing (nlp) technique used to parse the main topics of discussion in a set of document or a corpus. Topic Modeling can be used to organise and understand a large group of documents made of text data. There are multiple techniques to do this some easier to interpret than others such as Latent Semantic Analysis (LSA) and Latent Dirichlet Allocation (LDA)."},{"metadata":{"trusted":true},"cell_type":"code","source":"_np = np.asarray(sil_score)  #convert to np.array\n\n#set number of topics\ntopic_num = _np.argmax() +2\n\n#Create Corpus and Dictionary \ndictionary = corpora.Dictionary(articles['Full_Text'])\ncorpus = [dictionary.doc2bow(text) for text in articles['Full_Text']]\n\n#Create LDA model\nldamodel = LdaModel(corpus, num_topics=topic_num,id2word=dictionary, passes=30)\n\n#Display Topics \nlda_display = pyLDAvis.gensim.prepare(ldamodel, corpus,dictionary, sort_topics=False)\npyLDAvis.display(lda_display)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def topic_keywords(ldamodel,topic_num):\n    '''Extract topics from LDA model'''\n    keywords = {}\n    for i in range(topic_num):\n        keywords[i] = ', '.join(x[0] for x in ldamodel.show_topic(i))\n    \n    return keywords","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Extract the main topic for each document in the corpus\nmain_topic_prop = [sorted(article, key=lambda x: (x[1]), reverse=True)\\\n                   for i, article in enumerate(ldamodel[corpus])]\n\n#Dictionary of topics in LDA model\ntopics = topic_keywords(ldamodel,topic_num)\n\n#Create a Data Frame with the Main Topic and %score \ntopic_Dataframe = pd.DataFrame([(doc[0][0],doc[0][1],topics[doc[0][0]]) for doc in main_topic_prop],\\\n                             columns = ['Dominant_Topic', '% Score', 'Topic_Keywords'])\n#Concate Paper ID for articles\n\ntopic_Dataframe  = pd.concat([articles['Paper_Id'].reset_index(drop = True),topic_Dataframe],axis = 1)\ntopic_Dataframe ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Keyword search"},{"metadata":{"trusted":true},"cell_type":"code","source":"covid.keyword_Search(keywords = ['cough'])[:5]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Cosine Similarity"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics.pairwise import cosine_similarity\n\nsims = cosine_similarity(tfidf_matrix, tfidf_matrix)\n\nsimilarity_df = pd.DataFrame(list(sims)) # Create similarity dataframe\n\n#Change columns and index of the dataframe\nsimilarity_df.columns = articles['Paper_Id']\nsimilarity_df.index = articles['Paper_Id']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def similarity(paper_id_,sim =similarity_df,score = 0):\n    '''Acepsts \n        paper_id: ID of the Article\n        sim: similarity dataframe\n        score: minimum similarity score [0,1]\n        \n        Returns a pd.Series with of similarity scores with a minimum value score\n    '''\n    similar_articles = sim.loc[sim[paper_id_]>score,paper_id_]\n    return similar_articles.drop(paper_id_,axis = 0)\n\n#similarity(sim =similarity_df,paper_id_ = 'PMC7187821',score = 0.3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}